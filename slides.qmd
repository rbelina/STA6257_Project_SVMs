---
title: "Support Vector Machines"
author: "Robert Belina"
format: revealjs
---

## Introduction

-   Support Vector Machines (SVMs) have emerged as a powerful and versatile machine learning algorithm with applications spanning various domains.

-   Developed by Vapnik and colleagues in the 1990s, SVMs have gained significant popularity due to their ability to handle both classification and regression tasks effectively.

-   The fundamental concept behind SVMs is to find an optimal hyperplane that maximally separates different classes or fits the data points for regression, while simultaneously maintaining a clear margin between them [@kecman_2005_support].

## Brief Introduction to Machine Learning {.smaller}

-   Machine learning is the process of enabling computers to take actions by providing them with data and allowing them to discover patterns and insights autonomously, without explicit programming.

-   At the core of machine learning lies the importance of data.

-   Just as humans learn through information and data gathering, machines also require data to learn and make informed decisions.

\[[@brereton_2010_support]\]![](ml1.png){width="217"}

## Introduction

-   SVMs offer several advantages over traditional classification algorithms.

-   By emphasizing the importance of the margin, SVMs can effectively handle datasets with high dimensionality, noise, and outliers, resulting in superior performance in complex scenarios [@tian_2012_recent].

-   The distinguishing feature of SVMs lies in their ability to transform the original input space into a higher-dimensional feature space through the use of kernel functions.

## Introduction

-   SVMs provide a principled approach to handle both binary and multiclass classification problems. [@evgenybyvatov_2003_support]

-   The versatility of SVMs extends beyond just classification tasks [@brereton_2010_support].

photo here

## Methods

-   To begin understanding the use-cases of SVMs we will first look at the underlying math behind SVMs: linear algebra and a touch of optimization theory.

![](images/322.png){width="353"}

## Definitions

*Length of a Vector*

-   The norm of a vector x, denoted as \|\|x\|\|, represents its length. The Euclidean norm formula used to compute the norm of a vector x = (x1, x2, ..., xn) is as follows:

$$
||x||= √x21+x22+...+x2n
$$

## Definitions - Length and direction of a vector

*Vector Directions*

-   The direction of a vector x = (x1, x2) is denoted by w and is defined as follows:

$w = (x1/||x||,x2/||x||)$

-   Looking at the above, we can view the direction of the vector *w* as:

![](ml2.png)

## Definitions - Dot Product

-   The geometric formula for the dot product\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.01.48%20PM.png)

## Definitions - Dot Product

-   The algebraic formula of the dot product\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.05.14%20PM.png)

## Data Processing Methods for SVMs

-   Data Cleaning: Remove any irrelevant or redundant features, handle missing values, and address outliers if necessary.

-   Feature Scaling: Normalize the feature values to ensure that they have similar scales. Common scaling techniques include standardization (mean centering and scaling to unit variance) or normalization to a specific range.

-   Feature Selection: Select relevant features that contribute most to the prediction task, reducing dimensionality and improving model performance.

## Data Processing Methods for SVMs

-   Data Split: Divide the dataset into training and testing subsets. The training set is used to train the SVM model, while the testing set is used for evaluating its performance. Model Training:

-   Kernel Selection: Determine the appropriate kernel function based on the nature of the data and the problem at hand. Common kernel functions include linear, polynomial, Gaussian radial basis function (RBF), and sigmoid.

## Data Processing Methods for SVMs {.smaller}

-   Hyperparameter Tuning: Optimize the hyperparameters of the SVM model, such as the regularization parameter C and kernel-specific parameters like the degree of polynomial or the width of the RBF kernel. This can be done using techniques like grid search or cross-validation.

-   Model Fitting: Train the SVM model using the training dataset and the chosen hyperparameters. The goal is to find the optimal hyperplane or decision boundary that maximizes the margin between classes (in the case of classification) or minimizes the error (in the case of regression).

## Data Processing Methods for SVMs {.smaller}

-   Classification Metrics: Evaluate the performance of the SVM model for classification tasks using metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC).

-   Regression Metrics: Assess the performance of the SVM model for regression tasks using metrics such as mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared.

-   Cross-Validation: Perform k-fold cross-validation to estimate the model's generalization performance. This involves dividing the training dataset into k subsets, training the model on k-1 subsets, and evaluating its performance on the remaining subset. Repeat this process k times, rotating the evaluation subset each time.

## Data Processing Methods for SVMs {.smaller}

-   Model Selection: Compare the performance of different SVM models with varying hyperparameters or kernel functions to select the optimal model with the best performance on the testing data set.

-   Model Deployment: Once the SVM model has been trained and evaluated, it can be deployed to make predictions on new, unseen data. Preprocess the new data using the same steps as the training data (e.g., feature scaling), and apply the trained SVM model to classify or regress the new instances.

## Support Vector Machines in R {.scrollable}

-   Classification algorithm\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.20.16%20PM.png){width="657"}
-   Using the cricket player data set\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.25.00%20PM.png){width="401"}
-   Drawing the decision boundary\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.27.42%20PM.png){width="373"}
-   Margins matter\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.42.36%20PM.png){width="363"}
-   The correct decision boundary\
    ![](images/Screen%20Shot%202023-08-01%20at%2010.43.34%20PM.png){width="378"}\
    ![](images/Screen%20Shot%202023-08-01%20at%2011.05.09%20PM.png){width="378"}

## Working with Non-Linear Data and the Kernel Trick {.scrollable}

When working with non-linear data, we are often presented with more complex data sets with composite data points. One such example being:

![](images/dd.png){width="393"}

The solution is to add a third dimension, beyond only *x* and *y*:

$z = x² + y²$

Our new three dimensional space can now be viewed as:

![](images/Screen%20Shot%202023-07-30%20at%206.35.32%20PM.png){width="394"}

The new hyperplane for this three-dimensional dataset can now be viewed in two dimensions, from *below,* as:

![](images/Screen%20Shot%202023-07-30%20at%206.42.55%20PM.png){width="455"}

-   Separation with a simple line is impossible

-   A higher dimension is need\
    ![](images/Screen%20Shot%202023-08-01%20at%2011.01.30%20PM.png){width="619"}

## Data in Three Dimensions - Separated with a 2D Plane ![](images/Screen%20Shot%202023-08-01%20at%2011.12.05%20PM.png){width="489"}

## Analysis and Results

## Linear Support Vector Machine Classifier {.scrollable}

```{r}

set.seed(19327)
x = matrix(rnorm(80), 40, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)

library(e1071)

dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)

plot(svmfit, dat)

make.grid = function(x, n = 105) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}

xgrid = make.grid(x)
xgrid[1:10,]

ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("green","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)

beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho

plot(xgrid, col = c("green", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)

```

## Non-Linear Support Vector Machine Classifier
