<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Robert Belina">
<meta name="dcterms.date" content="2023-07-30">

<title>Support Vector Machines</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Support Vector Machines</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Robert Belina </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 30, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>Support Vector Machines (SVMs) have emerged as a powerful and versatile machine learning algorithm with applications spanning various domains. Developed by Vapnik and colleagues in the 1990s, SVMs have gained significant popularity due to their ability to handle both classification and regression tasks effectively. The fundamental concept behind SVMs is to find an optimal hyperplane that maximally separates different classes or fits the data points for regression, while simultaneously maintaining a clear margin between them <span class="citation" data-cites="kecman_2005_support">(<a href="#ref-kecman_2005_support" role="doc-biblioref">Kecman 2005</a>)</span>.</p>
<p>Machine learning is the process of enabling computers to take actions by providing them with data and allowing them to discover patterns and insights autonomously, without explicit programming.</p>
<p>At the core of machine learning lies the importance of data. Just as humans learn through information and data gathering, machines also require data to learn and make informed decisions.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ml1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">citation breteton</figcaption><p></p>
</figure>
</div>
<p>SVMs offer several advantages over traditional classification algorithms. Unlike methods that solely focus on minimizing training errors, SVMs are designed to maximize the margin between classes. This margin not only aids in achieving robust and accurate predictions on unseen data but also enhances the generalization ability of the model. By emphasizing the importance of the margin, SVMs can effectively handle datasets with high dimensionality, noise, and outliers, resulting in superior performance in complex scenarios <span class="citation" data-cites="tian_2012_recent">(<a href="#ref-tian_2012_recent" role="doc-biblioref">Tian, Shi, and Liu 2012</a>)</span>.</p>
<p>The distinguishing feature of SVMs lies in their ability to transform the original input space into a higher-dimensional feature space through the use of kernel functions. This enables SVMs to handle nonlinear relationships between variables without explicitly mapping them into the higher-dimensional space. By leveraging the kernel trick, SVMs efficiently capture complex decision boundaries, offering flexibility and adaptability to various data distributions. Furthermore, SVMs provide a principled approach to handle both binary and multiclass classification problems. Through techniques such as one-vs-one and one-vs-all, SVMs can extend their capabilities to accommodate multiple classes, ensuring accurate predictions across diverse scenarios <span class="citation" data-cites="evgenybyvatov_2003_support">(<a href="#ref-evgenybyvatov_2003_support" role="doc-biblioref">Byvatov and Schneider 2003</a>)</span></p>
<p>The versatility of SVMs extends beyond classification tasks, as they have also been successfully applied to regression, anomaly detection, and outlier detection problems. By employing support vector regression (SVR), SVMs can capture nonlinear relationships in continuous variables, making them well-suited for prediction tasks involving numerical outputs <span class="citation" data-cites="brereton_2010_support">(<a href="#ref-brereton_2010_support" role="doc-biblioref">Brereton and Lloyd 2010</a>)</span>.</p>
<p>In this review, we aim to provide an in-depth exploration of Support Vector Machines, covering their fundamental concepts, mathematical foundations, training algorithms, and diverse applications across various fields. By understanding the underlying principles and techniques of SVMs, researchers and practitioners can effectively leverage thi$$s powerful tool to tackle complex classification and regression problems, ultimately leading to enhanced predictive accuracy and insightful data analysis <span class="citation" data-cites="yang_2004_biological">(<a href="#ref-yang_2004_biological" role="doc-biblioref">Yang 2004</a>)</span>.</p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods">Methods</h2>
<p>To begin understanding the usecases of SVMs we will first look at the underlying math behind SVMs: linear algebra and a touch of optimization theory.</p>
<section id="definitions" class="level3">
<h3 class="anchored" data-anchor-id="definitions">Definitions</h3>
<p><em>Length of a Vector</em></p>
<p>The norm of a vector x, denoted as ||x||, represents its length. The Euclidean norm formula used to compute the norm of a vector x = (x1, x2, …, xn) is as follows:</p>
<p><span class="math display">\[
||x||= √x21+x22+...+x2n
\]</span></p>
<p><em>Vector Directions</em></p>
<p>The direction of a vector x = (x1, x2) is denoted by w and is defined as follows:</p>
<p><span class="math inline">\(w = (x1/||x||,x2/||x||)\)</span></p>
<p>Looking at the above, we can view the direction of the vector <em>w</em> as:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="ml2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption"><span class="citation" data-cites="burges_1998_a">(<a href="#ref-burges_1998_a" role="doc-biblioref">Burges 1998</a>)</span></figcaption><p></p>
</figure>
</div>
</section>
<section id="working-with-linear-data" class="level3">
<h3 class="anchored" data-anchor-id="working-with-linear-data">Working with Linear Data</h3>
<p>A more straightforward approach to understanding SVMs and their capabilities can be understood through illustration. Consider a scenario where we have two distinct categories, namely “blue” and “yellow” and our dataset consists of two features, denoted as “x” and “y.” Our objective is to develop a classifier that, when given a set of (x, y) coordinates, accurately predicts whether the point belongs to the “yellow” or “blue” category. To visualize this, we represent our labeled training data on a two-dimensional plane <span class="citation" data-cites="mammone_2009_support">(<a href="#ref-mammone_2009_support" role="doc-biblioref">Mammone, Turchi, and Cristianini 2009</a>)</span></p>
<p><img src="bb.png" class="img-fluid" width="346"></p>
<p>The support vector machine will take this data and interpret a hyperplane–here in this dimension will be a line–which will separate the data points. This separator is called the <em>decision boundary.</em> The decision is made as to whatever is on each side is either yellow or blue:</p>
<p><img src="cc.png" class="img-fluid" width="440"></p>
</section>
<section id="working-with-non-linear-data" class="level3">
<h3 class="anchored" data-anchor-id="working-with-non-linear-data">Working with Non-Linear Data</h3>
<p>When working with non-linear data, we are often presented with more complex data sets with composite data points. One such example being:</p>
<p><img src="images/dd.png" class="img-fluid" width="393"></p>
<p>The solution is to add a third dimension, beyond only <em>x</em> and <em>y</em>:</p>
<p><span class="math inline">\(z = x² + y²\)</span></p>
<p>Our new three dimensional space can now be viewed as:</p>
<p><img src="images/Screen%20Shot%202023-07-30%20at%206.35.32%20PM.png" class="img-fluid" width="394"></p>
<p>The new hyperplane for this three-dimensional dataset can now be viewed in two dimensions, from <em>below,</em> as:</p>
<p><img src="images/Screen%20Shot%202023-07-30%20at%206.42.55%20PM.png" class="img-fluid" width="455"></p>
</section>
<section id="working-with-kernel" class="level3">
<h3 class="anchored" data-anchor-id="working-with-kernel">Working with Kernel</h3>
<p>Above, we have explored a method to classify linear and nonlinear data by intelligently mapping our space to a higher dimension. However, performing this transformation can become computationally expensive, as it involves numerous new dimensions, each with potentially intricate calculations. Conducting this process for every vector in the dataset can be quite laborious, so it would be advantageous to discover a more cost-effective solution, hence the so-called kernel trick. Without incorporating the exact vectors, SVM will use the dot products between the dimensions, thus avoiding the need to calculate new dimensions <span class="citation" data-cites="benhur_2008_support">(<a href="#ref-benhur_2008_support" role="doc-biblioref">Ben-Hur et al. 2008</a>)</span></p>
<p><em>Define the new space</em></p>
<p><span class="math inline">\(z = x^2 + y^2\)</span></p>
<p><em>Define the dot product within this space</em></p>
<p><img src="aa.png" class="img-fluid"></p>
<p>We now have a less computationally expensive method to classify our data by utilizing various kernels to expand the feature space and accommodate non-linear boundaries between classes. Among the common kernel types used are: polynomial kernels, radial basis kernels, and linear kernels, the latter being equivalent to support vector classifiers. In summary, using the kernel method, the data is transformed to align with a linear hyperplane, enabling effective classification of the data.</p>
</section>
<section id="benefits-and-drawbacks" class="level3">
<h3 class="anchored" data-anchor-id="benefits-and-drawbacks">Benefits and Drawbacks</h3>
<p>Support Vector Machines (SVMs) are a robust machine learning algorithm used for classification and regression tasks. In this section, we describe the key steps involved in implementing SVMs, including data preprocessing, model training, and model evaluation, and some disadvantages to SVM.</p>
<p>Data Cleaning: Remove any irrelevant or redundant features, handle missing values, and address outliers if necessary. Feature Scaling: Normalize the feature values to ensure that they have similar scales. Common scaling techniques include standardization (mean centering and scaling to unit variance) or normalization to a specific range. Feature Selection: Select relevant features that contribute most to the prediction task, reducing dimensionality and improving model performance. Data Split: Divide the dataset into training and testing subsets. The training set is used to train the SVM model, while the testing set is used for evaluating its performance. <span class="citation" data-cites="sharma_2016_an">(<a href="#ref-sharma_2016_an" role="doc-biblioref">Sharma et al. 2016</a>)</span></p>
<p>Kernel Selection: Determine the appropriate kernel function based on the nature of the data and the problem at hand. Common kernel functions include linear, polynomial, Gaussian radial basis function (RBF), and sigmoid. Hyperparameter Tuning: Optimize the hyperparameters of the SVM model, such as the regularization parameter C and kernel-specific parameters like the degree of polynomial or the width of the RBF kernel. This can be done using techniques like grid search or cross-validation. Model Fitting: Train the SVM model using the training dataset and the chosen hyperparameters. The goal is to find the optimal hyperplane or decision boundary that maximizes the margin between classes (in the case of classification) or minimizes the error (in the case of regression). <span class="citation" data-cites="benhur_2008_support">(<a href="#ref-benhur_2008_support" role="doc-biblioref">Ben-Hur et al. 2008</a>)</span></p>
<p>Classification Metrics: Evaluate the performance of the SVM model for classification tasks using metrics such as accuracy, precision, recall, F1-score, and area under the receiver operating characteristic curve (AUC-ROC). Regression Metrics: Assess the performance of the SVM model for regression tasks using metrics such as mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared. Cross-Validation: Perform k-fold cross-validation to estimate the model’s generalization performance. This involves dividing the training dataset into k subsets, training the model on k-1 subsets, and evaluating its performance on the remaining subset. Repeat this process k times, rotating the evaluation subset each time. Model Selection: Compare the performance of different SVM models with varying hyperparameters or kernel functions to select the optimal model with the best performance on the testing dataset.</p>
<p>Once the SVM model has been trained and evaluated, it can be deployed to make predictions on new, unseen data. Preprocess the new data using the same steps as the training data (e.g., feature scaling), and apply the trained SVM model to classify or regress the new instances. <span class="citation" data-cites="benhur_2008_support">(<a href="#ref-benhur_2008_support" role="doc-biblioref">Ben-Hur et al. 2008</a>)</span></p>
</section>
</section>
<section id="analysis-and-data-visualization" class="level2">
<h2 class="anchored" data-anchor-id="analysis-and-data-visualization">Analysis and Data Visualization</h2>
<section id="linear-support-vector-machine-classifier" class="level3">
<h3 class="anchored" data-anchor-id="linear-support-vector-machine-classifier">Linear Support Vector Machine Classifier</h3>
<p>Support vector machines are best understood by looking at smaller sets of data, in this case, data we will formulate and incorporate ourselves. We begin by creating a dataset in two dimensions. Seed is set to random with forty observations among two classes with two variables. This will be our x matrix. For the y variable, we set it as one or negative one with ten in each class.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">19327</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">80</span>), <span class="dv">40</span>, <span class="dv">2</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">=</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>), <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">10</span>))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>x[y <span class="sc">==</span> <span class="dv">1</span>,] <span class="ot">=</span> x[y <span class="sc">==</span> <span class="dv">1</span>,] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Following, we load the e1071 library which contains the actual support vector machine function.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'e1071' was built under R version 4.1.2</code></pre>
</div>
</div>
<p>With our newly created data, we next create the DF, request the svm, and set the variables as non-standardized.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">data.frame</span>(x, <span class="at">y =</span> <span class="fu">as.factor</span>(y))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sft <span class="ot">=</span> <span class="fu">svm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">kernel =</span> <span class="st">"linear"</span>, <span class="at">cost =</span> <span class="dv">10</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We see below by printing the</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(sft)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
svm(formula = y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  linear 
       cost:  10 

Number of Support Vectors:  23</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(sft, dat)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>make.grid <span class="ot">=</span> <span class="cf">function</span>(x, <span class="at">n =</span> <span class="dv">105</span>) {</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  grange <span class="ot">=</span> <span class="fu">apply</span>(x, <span class="dv">2</span>, range)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  x1 <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> grange[<span class="dv">1</span>,<span class="dv">1</span>], <span class="at">to =</span> grange[<span class="dv">2</span>,<span class="dv">1</span>], <span class="at">length =</span> n)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  x2 <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from =</span> grange[<span class="dv">1</span>,<span class="dv">2</span>], <span class="at">to =</span> grange[<span class="dv">2</span>,<span class="dv">2</span>], <span class="at">length =</span> n)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">expand.grid</span>(<span class="at">X1 =</span> x1, <span class="at">X2 =</span> x2)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>xgrid <span class="ot">=</span> <span class="fu">make.grid</span>(x)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>xgrid[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>          X1        X2
1  -2.075718 -1.536734
2  -2.035796 -1.536734
3  -1.995873 -1.536734
4  -1.955950 -1.536734
5  -1.916028 -1.536734
6  -1.876105 -1.536734
7  -1.836183 -1.536734
8  -1.796260 -1.536734
9  -1.756337 -1.536734
10 -1.716415 -1.536734</code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>ygrid <span class="ot">=</span> <span class="fu">predict</span>(sft, xgrid)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"green"</span>,<span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(ygrid)], <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> .<span class="dv">2</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[sft<span class="sc">$</span>index,], <span class="at">pch =</span> <span class="dv">5</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-4-2.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">=</span> <span class="fu">drop</span>(<span class="fu">t</span>(sft<span class="sc">$</span>coefs)<span class="sc">%*%</span>x[sft<span class="sc">$</span>index,])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">=</span> sft<span class="sc">$</span>rho</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">"green"</span>, <span class="st">"blue"</span>)[<span class="fu">as.numeric</span>(ygrid)], <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> .<span class="dv">2</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">3</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x[sft<span class="sc">$</span>index,], <span class="at">pch =</span> <span class="dv">5</span>, <span class="at">cex =</span> <span class="dv">2</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(beta0 <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="sc">-</span>beta[<span class="dv">1</span>] <span class="sc">/</span> beta[<span class="dv">2</span>])</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((beta0 <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="sc">-</span>beta[<span class="dv">1</span>] <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>((beta0 <span class="sc">+</span> <span class="dv">1</span>) <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="sc">-</span>beta[<span class="dv">1</span>] <span class="sc">/</span> beta[<span class="dv">2</span>], <span class="at">lty =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-4-3.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="non-linear-support-vector-machine-classifier" class="level3">
<h3 class="anchored" data-anchor-id="non-linear-support-vector-machine-classifier">Non-Linear Support Vector Machine Classifier</h3>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(e1071)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="at">file =</span> <span class="st">"ESL.mixture.rda"</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(ESL.mixture)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "x"        "y"        "xnew"     "prob"     "marginal" "px1"      "px2"     
[8] "means"   </code></pre>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="fu">rm</span>(x, y)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="fu">attach</span>(ESL.mixture)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">y =</span> <span class="fu">factor</span>(y), x)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">=</span> <span class="fu">svm</span>(<span class="fu">factor</span>(y) <span class="sc">~</span> ., <span class="at">data =</span> dat, <span class="at">scale =</span> <span class="cn">FALSE</span>, <span class="at">kernel =</span> <span class="st">"radial"</span>, <span class="at">cost =</span> <span class="dv">5</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> px1, <span class="at">X2 =</span> px2)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>ygrid <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid)     </span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">as.numeric</span>(ygrid), <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> .<span class="dv">2</span>)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid" width="672"></p>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>func <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid, <span class="at">decision.values =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>func <span class="ot">=</span> <span class="fu">attributes</span>(func)<span class="sc">$</span>decision</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>xgrid <span class="ot">=</span> <span class="fu">expand.grid</span>(<span class="at">X1 =</span> px1, <span class="at">X2 =</span> px2)</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>ygrid <span class="ot">=</span> <span class="fu">predict</span>(fit, xgrid)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(xgrid, <span class="at">col =</span> <span class="fu">as.numeric</span>(ygrid), <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">cex =</span> .<span class="dv">2</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="fu">points</span>(x, <span class="at">col =</span> y <span class="sc">+</span> <span class="dv">1</span>, <span class="at">pch =</span> <span class="dv">19</span>)</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(func, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="dv">0</span>, <span class="at">add =</span> <span class="cn">TRUE</span>)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="fu">contour</span>(px1, px2, <span class="fu">matrix</span>(func, <span class="dv">69</span>, <span class="dv">99</span>), <span class="at">level =</span> <span class="fl">0.5</span>, <span class="at">add =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">"green"</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-5-3.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">

</div>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>…Additionally, the SVM model was tested on an independent test dataset to assess its generalization performance. The results showed consistent and reliable performance, further validating the effectiveness of the SVM model.</p>
<p>Overall, the results highlight the capabilities of SVMs as a powerful machine learning algorithm for classification tasks. The SVM model successfully classified the target variable with high accuracy and demonstrated robustness in handling complex relationships within the data. These findings emphasize the potential of SVMs as a valuable tool for data analysis and prediction in various domains.</p>
</section>
<section id="statistical-modeling" class="level3">
<h3 class="anchored" data-anchor-id="statistical-modeling">Statistical Modeling</h3>
</section>
<section id="conlusion" class="level3">
<h3 class="anchored" data-anchor-id="conlusion">Conlusion</h3>
<p>In conclusion, this paper has provided a comprehensive overview of Support Vector Machines (SVMs) and their application in the field of machine learning. SVMs have proven to be a powerful and versatile algorithm for tackling classification and regression tasks. Their effectiveness in high-dimensional spaces and ability to handle cases with more features than samples make them a valuable tool in various real-world applications.</p>
<p>Throughout the paper, we discussed the fundamental concepts of SVMs, including hyperplanes, support vectors, and the importance of maximizing the margin for better classification performance. We also explored the use of kernel functions to handle non-linear data and the significance of hyperparameter tuning to prevent overfitting and achieve optimal results.</p>
<p>Moreover, we highlighted the advantages of SVMs, such as their memory efficiency and robustness in dealing with complex datasets. Additionally, SVMs offer the flexibility to use various kernel functions, enabling users to adapt the algorithm to different problem domains.</p>
<p>However, we also discussed some limitations of SVMs, such as the lack of direct probability estimates and the computational expense associated with cross-validation for probability calculation. It is crucial to carefully select appropriate kernel functions and regularization terms, especially when dealing with high-dimensional data to prevent overfitting.</p>
<p>Overall, SVMs continue to be a widely used and highly regarded algorithm in the machine learning community due to their accuracy, effectiveness, and versatility. Their ability to handle various classification tasks and perform well in both linear and non-linear scenarios makes them a valuable asset in the data scientist’s toolkit. As machine learning continues to evolve, SVMs will remain an essential component for addressing complex classification challenges in diverse applications. Further research and development in this area hold the promise of expanding SVMs’ capabilities and advancing their practical implementation in various domains.</p>
</section>
</section>
<section id="references" class="level2 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-benhur_2008_support" class="csl-entry" role="doc-biblioentry">
Ben-Hur, Asa, Cheng Soon Ong, Sören Sonnenburg, Bernhard Schölkopf, and Gunnar Rätsch. 2008. <span>“Support Vector Machines and Kernels for Computational Biology.”</span> Edited by Fran Lewitter. <em>PLoS Computational Biology</em> 4 (October): e1000173. <a href="https://doi.org/10.1371/journal.pcbi.1000173">https://doi.org/10.1371/journal.pcbi.1000173</a>.
</div>
<div id="ref-brereton_2010_support" class="csl-entry" role="doc-biblioentry">
Brereton, Richard G., and Gavin R. Lloyd. 2010. <span>“Support Vector Machines for Classification and Regression.”</span> <em>The Analyst</em> 135: 230–67. <a href="https://doi.org/10.1039/b918972f">https://doi.org/10.1039/b918972f</a>.
</div>
<div id="ref-burges_1998_a" class="csl-entry" role="doc-biblioentry">
Burges, Christopher J. C. 1998. <span>“A Tutorial on Support Vector Machines for Pattern Recognition.”</span> <em>Data Mining and Knowledge Discovery</em> 2: 121–67. <a href="https://doi.org/10.1023/a:1009715923555">https://doi.org/10.1023/a:1009715923555</a>.
</div>
<div id="ref-evgenybyvatov_2003_support" class="csl-entry" role="doc-biblioentry">
Byvatov, Evgeny, and Gisbert Schneider. 2003. <span>“Support Vector Machine Applications in Bioinformatics.”</span> <em>Applied Bioinformatics</em> 2 (January): 67–77.
</div>
<div id="ref-kecman_2005_support" class="csl-entry" role="doc-biblioentry">
Kecman, V. 2005. <span>“Support Vector Machines – an Introduction.”</span> <em>Support Vector Machines: Theory and Applications</em> 177 (April): 1–47. <a href="https://doi.org/10.1007/10984697_1">https://doi.org/10.1007/10984697_1</a>.
</div>
<div id="ref-mammone_2009_support" class="csl-entry" role="doc-biblioentry">
Mammone, Alessia, Marco Turchi, and Nello Cristianini. 2009. <span>“Support Vector Machines.”</span> <em>Wiley Interdisciplinary Reviews: Computational Statistics</em> 1 (November): 283–89. <a href="https://doi.org/10.1002/wics.49">https://doi.org/10.1002/wics.49</a>.
</div>
<div id="ref-sharma_2016_an" class="csl-entry" role="doc-biblioentry">
Sharma, Vikas, Diganta Baruah, Dibyajyoti Chutia, N Raju, and D Bhattacharya. 2016. <span>“An Assessment of Support Vector Machine Kernel Parameters Using Remotely Sensed Satellite Data,”</span> May. <a href="https://doi.org/10.1109/rteict.2016.7808096">https://doi.org/10.1109/rteict.2016.7808096</a>.
</div>
<div id="ref-tian_2012_recent" class="csl-entry" role="doc-biblioentry">
Tian, Yingjie, Yong Shi, and Xiaohui Liu. 2012. <span>“RECENT ADVANCES ON SUPPORT VECTOR MACHINES RESEARCH.”</span> <em>Technological and Economic Development of Economy</em> 18 (April): 5–33. <a href="https://doi.org/10.3846/20294913.2012.661205">https://doi.org/10.3846/20294913.2012.661205</a>.
</div>
<div id="ref-yang_2004_biological" class="csl-entry" role="doc-biblioentry">
Yang, Z. R. 2004. <span>“Biological Applications of Support Vector Machines.”</span> <em>Briefings in Bioinformatics</em> 5 (January): 328–38. <a href="https://doi.org/10.1093/bib/5.4.328">https://doi.org/10.1093/bib/5.4.328</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>